---
layout: post
title: Combining streams and text processing with Apache Flink and OpenNLP
subtitle: The unusual world of machine learning outside Python
tags: NLP streams
references:
    - id: flink
      author: Apache Software Foundation
      title: Apache Flink
      year: 2014-2019
      url: https://flink.apache.org
    - id: corenlp
      author: Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David
      title: The Stanford CoreNLP Natural Language Processing Toolkit
      book-title: Association for Computational Linguistics (ACL) System Demonstrations
      year: 2014
      pages: 55--60
      url: https://stanfordnlp.github.io/CoreNLP/
    - id: opennlp
      author: Apache Software Foundation
      title: "OpenNLP: a Java NLP library"
      year: 2004-2019
      url: https://opennlp.apache.org/
    - id: lemmatizer
      author: richardwilly98
      title: ElasticSearch OpenNLP auto-tagging
      url: https://github.com/richardwilly98/elasticsearch-opennlp-auto-tagging
    - id: universal-dep
      author: LINDAT / CLARIAH-CZ Data & Tools
      title: Universal Dependencies 2.5
      year: 2019
      url: https://universaldependencies.org/
---

A spot on the podium for the most trending technologies in the current software industry should probably be reserved to real-time stream processing. In recent years, a plethora of tools and frameworks have seen the light, with many of them becoming part of the Apache Software Foundation, such as the popular Spark and his "little brother" Flink.
The former was initially conceived as a natural evolution of Hadoop with in-memory functionality, a general-purpose distributed computing tool on large amounts of data, but it has gradually been adapted for on-the-fly computation using mini-batches.
On the other hand, the latter was purposefully designed for stateful and distributed streams, becoming the perfect candidate to process information coming from different sources or services with minimal latency while keeping the overall system as much responsive as possible.

Apart from batches (which are still an option, by the way) Flink's _modus operandi_ is absolutely comparable with Spark: each program can be seen as a single job that can be compiled, packaged in a jar file and executed in an environment, which in turn represents an abstraction for a cluster of physical or virtual machines sharing the work.
Leaving the details of the underlying architecture, well documented on the [official site](#ref-flink), a single job is nothing more than a combination of basic building blocks called _operators_, starting from one or multiple sources and ending with at least a single output, called _sink_.

```java
// Toy example of Twitter source, flat map and RabbitMQ sink
// The support for lambda functions is pretty neat
env.addSource(new QueuedTwitterSource(properties))
  .name("source-twitter")
  .flatMap(s -> TwitterObjectFactory.createStatus(s))
  .name("flat-map-tweet");
  .addSink(new RabbitMQSink<>())
  .name("sink-rabbit");
```

## Keeping it light
Having recently dealt with Twitter's streaming API, I took the opportunity to experiment with this excellent tool in its Java version and, if there was any doubt, I'm glad to report that it has been a pleasure, _for the most part_: the operator-oriented flow using a functional style is really intuitive, it is so simple to manage that you could probably condense most of your work in a few lines of code using lambdas.
In terms of adaptability, Flink provides off-the-shelf connectors for most of today's most popular brokers and distributed queues, such as Kafka  or RabbitMQ, while also offering simpler and less obvious choices such as database connectors or low-level file readers and writers. Expanding or creating your own stream source is as simple as implementing the _SourceFunction<T>_ interface or extending the _RichSourceFunction<T>_, if you need more freedom. The Twitter source included with Flink it's a neat and straightforward example of this, even if I'd suggest rolling your own implementation in this specific case since it pretty much lacks any kind of event handling from the HTTP client.

So far so good, then why did I say _for the most part_? Well, if your purpose is to use Flink as a data gathering and processing platform without any particular requirements you're pretty much right on track. However, if you're a stubborn machine learning addict, prepare for a steeper learning curve.  
Unfortunately, compared to its bigger brother Spark, Flink's ML resources are still pretty limited and at the time of writing the Python API is still in development, therefore most of the heavy lifting has to be delegated to third-party libraries.

In my particular case, I required some way of filtering incoming tweets in real time, keeping only the ones pertinent with the keywords specified in the source. Of course, I could have just went on and implemented my own solution, I could have probably gotten away with a dictionary-based lookup system, but I took the opportunity to explore the natural language processing world outside of NLTK, Spacy and the deep monsters (not that there's anything wrong with any of that, on the contrary).  
Dealing with a typical NLP problem in a Java environment, the first quite obvious option is the popular [Stanford CoreNLP library](#ref-corenlp), which I immediately downloaded and added to the maven project. The framework is clearly stable, solid and well maintained by a large community, the documentation is concise but at the same time exhaustive and the performance is unquestionable, almost certainly the best available at the moment for Java software.

If you even end up with a text processing problem to solve in Java using machine learning I recommend you to give CoreNLP a try. However, for my specific use case, I wasn't completely satisfied with what I had for a couple of reasons: the first one cannot be considered a problem _per se_, but rather an issue I had with the verbosity of the library, which I will admit bothered me more than usual probably because of the contrast with Flink's cleaner looks. I am certain there were valid reasons behind such choices, but in my humble opinion a wrapper class for every annotation is not ideal and probably counter-intuitive, making the code hardly readable at a first glance.  
In any case, this first motivation is quite forgivable and could be very well blamed on my little patience. On the other hand, the second issue I encountered was more practical and critical: the CoreNLP framework provides general-purpose models with excellent performance, which unfortunately come at a cost of a quite heavy package.
The standard package alone weighs around 400MB, that is without taking into consideration the additional packages with the latest models subdivided by language.  
Again, this is a minor problem, nevertheless I required something slimmer that I could easily fit inside a scalable Docker container, so I started my journey again and I stumbled upon another very good candidate, [Apache OpenNLP](#ref-opennlp).

## Exploring Apache OpenNLP
Compared to Stanford's CoreNLP, this framework would certainly look like a step back in every way at first sight: the latter offers a wider variety of models and techniques, which for the most part probably perform better, without mentioning the available support that, in the case of OpenNLP, is pretty much confined to the official documentation.
However, a couple of trials with this library can easily reveal its strengths: the API is pretty straightforward, there is no explicit need to comply with a rigid pipeline if your project doesn't require it. Besides, the library itself is very extensible and allows for easier interactions with other frameworks such as Flink.  
As detailed in the manual, the same overall structure is shared across the whole library: at the top level, the building blocks are represented by _tools_ which can be instantiated with its own parameters. Every tool can usually execute a single NLP _task_, such as tokenization, requiring specific inputs and providing its own outputs, that usually consist of string or arrays of strings. In order to for the tools to work, the latter require a _model_ which can be easily read as a standard _InputStream_.

```java
// deserialize the model, load it into the tool and you're ready
try (InputStream input = newFileInputStream("tokenizer-en.bin")){
  TokenizerModel model = new TokenizerModel(input);
  TokenizerME tokenizer = new TokenizerME(model);
  String[] tokens = tokenizer.tokenize("Awesome movie, give it a try!");
} ...
```
As most frameworks out there, OpenNLP also offers a convenient CLI that offers quick access to both training and evaluation of the tools. For instance, a quick way to train a tagger could be the following simple command:

```shell
opennlp POSTaggerTrainer -model pos-en.bin -data pos.train.txt -lang en -encoding UTF-8
```

The parameter _model_ specifies the path and filename of the serialized output model, _data_ simply indicates the source containing the annotated corpus, while the last two are self-explanatory. The training tools, while providing the same interface, are very flexible and offer almost the same kind of freedom of the API. You can find the complete list of parameters on the [official manual](https://opennlp.apache.org/docs/1.9.2/manual/opennlp.html), together with some usage examples.  

But wait, what kind of data should I provide to the trainers? Well, that's a question that made me struggle more than I'm willing to admit.
Technically, every tool should be able to read CoNLL formatted data, although I couldn't manage to make it work, despite checking structures and encoding of my corpora.
Luckily, the manual details the format that the input files should have for each tool which, for some unknown reason, is always different. The following table summarizes the structure for some of the available tools:

| Tool | Format | Example |
|------|--------|---------|
|Tokenizer | A single sentence per line, each token separated by space or `<SPLIT>` tag | `We do<SPLIT>n't have to believe him<SPLIT>.` |
| POS Tagger | A single sentence per line composed of [token]_[PoS tag] separated by spaces | `We_PRON do_AUX n't_PART have_VERB to_PART believe_VERB him_PRON ._PUNCT` |
| Trainable Lemmatizer | for each line, token, PoS tag and lemma separated by tabs | `n't PART	not` |
| Dictionary Lemmatizer | same as statistical version, but the token is used as key | [example file](https://raw.githubusercontent.com/richardwilly98/elasticsearch-opennlp-auto-tagging/master/src/main/resources/models/en-lemmatizer.dict) |
| Document Categorizer | one sentence per line with label, tab and space-separated tokens | `NEGATIVE  We do n't have to believe him .` |

The example dictionary, provided in this [example project](#ref-lemmatizer) by the way, can be used as both train set for the statistical version and lookup table for the dictionary lemmatizer. Again, this is not meant as an exhaustive list (check the manual for the specific tool you require), but it can give you an idea of the file formats you could provide to build a simple preprocessing pipeline.  
For the sake of simplicity, you can have a look at this [simple script](https://gist.github.com/edoarn/01201caf3d98ce82e6dea736b86479d7) that I wrote to convert a CoNLL file into different sets compatible with each OpenNLP tool I mentioned above.
The code has been tested on corpora provided by [Universal Dependencies](#ref-universal-dep) which provide slightly different annotations compared to the old-school Peen treebank for instance, but it should be compatible with any variation, as long as it is a valid CoNLL file.

Once you generated the respective input files, you should be able to run the training tools even from the command line and obtain something similar to this:

```
opennlp POSTaggerTrainer -model .\models\en-pos.bin -lang en -data .\data\pos-en-train.txt -encoding UTF-8  
>> Indexing events with TwoPass using cutoff of 5  
>>   
>>       Computing event counts...  done. 317900 events  
>>       Indexing...  done.  
>> Sorting and merging events... done. Reduced 317900 events to 299258.  
>> Done indexing in 9,17 s.  
>> Incorporating indexed data for training...  
>> done.  
>>         Number of Event Tokens: 299258  
>>         Number of Outcomes: 18  
>>         Number of Predicates: 46384  
>> ... done.  
>> Computing model parameters ...  
>> Performing 100 iterations.   
>> ...  
>> 100: ... loglikelihood=-29097.986584811264     0.9823718150361749  
>> Writing pos tagger model ... done (1,034s)  
>> Wrote pos tagger model to  
>> path: <full-path>\en-pos.bin  
  
opennlp POSTaggerEvaluator -model .\models\en-pos.bin -data .\data\pos-en-test.txt -encoding UTF-8 
>> Loading POS Tagger model ... done (0,255s)  
>> Evaluating ... done  
>> 
>> Accuracy: 0.9259109220624311  
>> Execution time: 1,625 seconds  
```

As you can see it's far from perfect, but for a simple training without any parameter tuning we can settle for a 92% of accuracy on PoS tagging. For quick reference, the test file contained roughly 50.000 tokens with their respective tags, so the tagger operated on around _30k_ tags/sec on an old 3.5Ghz CPU, not too bad!
Even more interesting, the output of the _bin_ file is small enough to be stored inside the _resources_ folder of our Flink project, so we can read the model in the main method and pass it along to the function for execution.

## Categorizing documents
Until now I only talked about preprocessing the text, but OpenNLP also provides basic albeit practical models for tasks such as Named Entity Recognition (NER) or simple sentiment analysis through text classification.  
For the latter, the corresponding tool goes by the name of _Document Categorizer_ or _Doccat_ for short.
As usual, the classifier can theoretically be trained via CLI using the appropriate parameters similarly to previous tools, however it can be quite cumbersome to test different parameters using this procedure, so I opted for the API alternative.
As a quick example, the following Java extract shows how to setup a DoccatModel:

```java
//the corpus is a list of strings formatted as specified in the table
String language = "en";
List<String> corpus = ...
// preprocess and format your text

// Setup for training:
// Create a wrapper stream from the collection
DocumentSampleStream data = new DocumentSampleStream(new CollectionObjectStream<>(corpus));
// Create default parameters, then update as required
TrainingParameters params = ModelUtil.createDefaultTrainingParameters();
params.put(TrainingParameters.ALGORITHM_PARAM, "MAXENT");
params.put(TrainingParameters.CUTOFF_PARAM, 10);
params.put(TrainingParameters.ITERATIONS_PARAM, 100);
// Add feature generators
DoccatFactory factory = new DoccatFactory(new FeatureGenerator[]{new BagOfWordsFeatureGenerator()});

// Train and save your model
DoccatModel model = DocumentCategorizerME.train(language, data, params, factory);
model.serialize(new File(String.format("doccat-maxent-{}.bin", language)));
```

As you can see, this is as simple as it gets: the whole procedure roughly follows the same standards of the CLI, where you can define the language, provide the data and the parameters, and then serialize and save the resulting model to file.
Using this setup however you can easily experiment with different combinations of algorithms, parameters or feature generators.
Specifically, limited to the document categorizer OpenNLP provides implementations for a handful of relatively simple algorithms, grouped in the list below:
 
 - **MAXENT:** A model based on Maximum Entropy.
 - **MAXENT_QN:** Similar, but using a quasi-Newton method for inference.
 - **PERCEPTRON:** A standard perceptron algorithm.
 - **NAIVEBAYES:** Naive Bayes implementation mixed with EM.
 - **PERCEPTRON_SEQUENCE:** A variation of the perceptron dedicated to sequences.

I won't extend this already long scroll with the details of each algorithm (you can take a look on the [official GitHub repository](https://github.com/apache/opennlp/tree/master/opennlp-tools/src/main/java/opennlp/tools/ml)), but from my little experience I can affirm that most of them should perform equally well in most cases.
When in doubt, MaxEnt or Naive Bayes are typically a robust yet fast choice for text categorization with this library.  
In terms of other parameters, the most significant ones are `CUTOFF_PARAM` and `ITERATIONS_PARAM`. The former represents the minimum _cutoff_ frequency that the model should use on the input documents: every term with a total count under the given threshold will be discarded.
This is crucial for BoW-based models since an excessive cutoff could quickly disrupt the performance by removing important tokens, while, at the same time, a value too small could lead to very sparse and inefficient feature vectors due to the high dimensionality of the data.
The latter is instead quite self-explanatory as it indicates the maximum number of iterations before stopping the training procedure, although it is worth mentioning that some of the algorithms have other convergence thresholds and could easily stop before the established mark.  
Last but not least, you can provide different feature generators to improve the performance using the input array provided to the _DoccatFactory_ instance.
OpenNLP only provides _BagOfWordFeatureGenerator_ and _NGramFeatureGenerator_ implementations, nevertheless you could implement and provide your own generator if you wish.

Lastly, let's take a look at the evaluation of the model just trained.
``` java
// Load the model as input stream
try (InputStream modelIn = new FileInputStream("doccat-maxent-en.bin")) {
  // Initialize document categorizer tool
  DocumentCategorizerME classifier = new DocumentCategorizerME(new DoccatModel(modelIn));

  // iterate over the test set: assume that we tokenized and joined the tokens again with spaces
  for (String example : testCorpus) {
    String[] tokens = example.split(" ");
    // categorize returns probabilities for every class
    double[] pred = classifier.categorize(tokens);
    // compute metrics or use the output in other ways
    ...
} catch (IOException e)  {
  //please don't judge this absolutely perfect error handling
  e.printStackTrace();
}
```

Once the models have been trained and tested, they can be easily integrated inside a Flink job.
Once you made sure to add the required dependencies to your Maven or Gradle project, importing and using these tools is completely straightforward: load the model as a resource, provide it to your custom operator and finally assign it to the actual executor.

For instance, in the job's _main method_:
```java
// load models
tokenizer = new TokenizerModel(Application.class.getResource("token-en.bin"));
classifier = new DoccatModel(Application.class.getResource("doccat-en.bin"));
...
// define the stream
stream
  .map(new TokenizerFunction(tokenizer))
  .map(new DoccatFunction(classifier))
  .addSink(new RabbitMQSink(properties));
```
While, inside the function class, the loaded model can be fed to the actual tool in the _open_ method, which in turn can be called inside the _map_ iterations:

``` java
public class TokenizerFunction extends RichMapFunction<String,Annotation> {

    private transient Tokenizer tokenizer;
    private final TokenizerModel model;

    public TokenizerFunction(final TokenizerModel model) {
        // the model is provided at definition time
        // so that it can be serialized inside the job and instantiated on open
        this.model = model;
    }

    @Override
    public void open(Configuration parameters) throws Exception {
      // important to load the model just once at startup
        tokenizer = new TokenizerME(model);
    }

    @Override
    public Annotation map(String text) throws Exception {
      // simple example using a wrapper class, but you could get fancy with this
      return new Annotation(tokenizer.tokenize(text))
    }
}
```

This is just meant to be a sort of demonstration, an actual NLP pipeline aimed at text classification could, for starters, include some text cleaning and normalization (e.g. removing mentions and retweets from a tweet), a classical tokenization pass, PoS tagging, lemmatization or stemming (even just a dictionary lookup could be worth it) or even some more advanced methods such as word embeddings.
This is my not-so-short summary on how to start with OpenNLP in a distributed stream context such as Flink. Again, this is just one working solution: not the best, probably not the worst, but there could be other efficient and clever ways to do the same task, so keep experimenting!